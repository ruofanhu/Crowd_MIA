from flair.data import Corpus
from data_fetcher import NLPTaskDataFetcher
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings, PooledFlairEmbeddings
from flair.trainers import ModelTrainer
from flair.models import SequenceTagger
from typing import List
import argparse
import os
from sequence_tagger_with_weights import WeightedSequenceTagger
import json
import datetime
from nervaluate import Evaluator
import seqeval.metrics
from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score

parser = argparse.ArgumentParser()
parser.add_argument('--data_folder_path')
parser.add_argument('--model_folder_path')
parser.add_argument('--test_file_name')
parser.add_argument('--label_map_name')
parser.add_argument("--performance_file", default='all_test_performance.txt', type=str)


args = parser.parse_args()
print(vars(args))

column_format = {0: 'text', 1: 'ner'} # the datafiles generated by our scripts have columns: text ner [weight]
# if args.include_weight:
#     column_format[2] = 'weight'
    
# this can be modified to individual needs.
data_folder = args.data_folder_path
model_folder = args.model_folder_path

with open(os.path.join(data_folder, args.label_map_name), 'r') as fp:
    label_map = json.load(fp)
    
test_corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder,
                                                            column_format=column_format,
                                                            tag_to_biloes="ner",
                                                            test_file=args.test_file_name,
                                                            train_file=args.test_file_name,
                                                            dev_file=args.test_file_name)

    
def compute_metrics(pred_list, label_list, label_map):
    labels = list(label_map.keys())
    labels = [i[2:] for i in labels if i.startswith('B-')]
    evaluator = Evaluator(label_list, pred_list, tags=labels, loader="list")
    results, results_by_tag = evaluator.evaluate()
    try:
        cls_report = seqeval.metrics.classification_report(label_list, pred_list, zero_division=1)
    except:
        cls_report = ""
    return {
        "accuracy_score": seqeval.metrics.accuracy_score(label_list, pred_list),
        "results": results,
        "results_by_tag": results_by_tag,
        "CR": cls_report,
    }


def validate_bio(labels):
    for cur_label, next_label in zip(labels, labels[1:] + ['O']):
        if cur_label[0] == 'O':
            assert next_label[0] == 'O' or next_label[0] == 'B'
            continue
        elif cur_label[0] == 'B':
            assert next_label[0] == 'O' or next_label[0] == 'B' or (
                        next_label[0] == 'I' and cur_label[1:] == next_label[1:])
        elif cur_label[0] == 'I':
            assert next_label[0] == 'O' or next_label[0] == 'B' or \
                   (next_label[0] == 'I' and cur_label[1:] == next_label[1:])
        else:
            assert False
            

# loads a column dataset into list of (tokens, labels)
# assumes BIO(IOB2) labeling
def load_dataset_from_column(path, schema='bio'):
    with open(path, 'r', encoding='utf-8') as f:
        sentences = []
        tokens = []
        labels = []
        for line in f.readlines() + ['']:
            if len(line) == 0 or line.startswith('-DOCSTART-') or line.isspace():
                if len(tokens) > 0:
                    if schema is not None and schema != 'none':
                        if schema == 'iob':
                            labels = iob2bio(labels)
                        elif schema == 'iobes':
                            labels = iobes2bio(labels)
                        validate_bio(labels)
                    sentences.append((tokens, labels))
                tokens = []
                labels = []
            else:
                splits = line.strip().split()
                token, label = splits[0], splits[-1]
                tokens.append(token)
                labels.append(label)
    return sentences


def get_tokens_and_labels(sentence):
    tokens = []
    labels = []
    for token in sentence.tokens:
        tokens.append(token.text)
        labels.append(token.get_tag("ner").value)
    return tokens, labels


def iobes2bio(iobes_labels):
    bio_labels = []
    for label in iobes_labels:
        if label[0] == 'S':
            bio_labels.append('B' + label[1:])
        elif label[0] == 'E':
            bio_labels.append('I' + label[1:])
        else:
            bio_labels.append(label)
    return bio_labels


tagger = SequenceTagger.load(os.path.join(model_folder, 'final-model.pt'))

test_sentences = [x for x in test_corpus.test]
tagger.predict(test_sentences)
sentences = []
for sentence in test_sentences:
    tokens, labels = get_tokens_and_labels(sentence)
    labels = iobes2bio(labels)
    sentences.append((tokens, labels))

test_data = load_dataset_from_column(os.path.join(data_folder, args.test_file_name))

label_list = [i[1] for i in test_data]
pred_list = [i[1] for i in sentences]

results_dict = compute_metrics(pred_list, label_list, label_map)

performance_dict = vars(args)
for k, v in results_dict.items():
    performance_dict['T_best_test_'+k] = v

performance_dict['script_file'] = os.path.basename(__file__)
performance_dict['Time'] = str(datetime.datetime.now())

with open(args.performance_file, 'a+') as outfile:
    outfile.write(json.dumps(performance_dict) + '\n')



